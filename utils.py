import pandas as pd
import numpy as np
import streamlit as st
from typing import Tuple, List, Optional, Union
import logging
from datetime import datetime
import os
import pickle
import openai
from dotenv import load_dotenv

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

logger = setup_logging()

def validate_dataframe(df: pd.DataFrame, ticker: str, required_columns: List[str]) -> bool:
    """ë°ì´í„°í”„ë ˆì„ ìœ íš¨ì„± ê²€ì‚¬"""
    if df is None:
        st.error(f"'{ticker}' ë°ì´í„°ê°€ Noneì…ë‹ˆë‹¤.")
        return False
        
    if df.empty:
        st.error(f"'{ticker}' ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return False
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        st.error(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_columns}")
        return False
    
    if not isinstance(df.index, pd.DatetimeIndex):
        st.error(f"'{ticker}' ë°ì´í„°ì˜ ì¸ë±ìŠ¤ê°€ ë‚ ì§œ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. (í˜„ì¬ íƒ€ì…: {type(df.index)})")
        return False
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
    null_counts = df[required_columns].isnull().sum()
    if null_counts.any():
        st.warning(f"'{ticker}' ë°ì´í„°ì— ê²°ì¸¡ê°’ì´ ìˆìŠµë‹ˆë‹¤: {null_counts[null_counts > 0].to_dict()}")
    
    # ìµœì†Œ ë°ì´í„° ê°œìˆ˜ í™•ì¸
    if len(df) < 50:
        st.error(f"'{ticker}' ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. (í˜„ì¬: {len(df)}ê°œ, ìµœì†Œ í•„ìš”: 50ê°œ)")
        return False
    
    return True

def normalize_timezone(df: pd.DataFrame, target_tz: str = 'Asia/Seoul') -> pd.DataFrame:
    """íƒ€ì„ì¡´ ì •ê·œí™”"""
    try:
        if df.index.tz is None:
            df.index = df.index.tz_localize('UTC').tz_convert(target_tz)
        else:
            df.index = df.index.tz_convert(target_tz)
        return df
    except Exception as e:
        logger.warning(f"íƒ€ì„ì¡´ ì •ê·œí™” ì‹¤íŒ¨: {e}")
        return df

def clean_data(df: pd.DataFrame, required_columns: List[str]) -> pd.DataFrame:
    """ë°ì´í„° ì •ë¦¬ ë° ì „ì²˜ë¦¬"""
    df_clean = df.copy()
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬ (forward fill í›„ backward fill)
    df_clean[required_columns] = df_clean[required_columns].ffill().bfill()
    
    # ì´ìƒê°’ ì²˜ë¦¬ (ê° ì»¬ëŸ¼ë³„ë¡œ 99.5% ë¶„ìœ„ìˆ˜ë¡œ ìº¡í•‘)
    for col in required_columns:
        if col != 'Volume':  # Volumeì€ 0ì´ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œì™¸
            upper_bound = df_clean[col].quantile(0.995)
            lower_bound = df_clean[col].quantile(0.005)
            df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)
    
    # Volumeì´ 0ì¸ ê²½ìš° ì²˜ë¦¬
    if 'Volume' in required_columns:
        df_clean['Volume'] = df_clean['Volume'].replace(0, df_clean['Volume'].median())
    
    return df_clean

def calculate_returns(df: pd.DataFrame, price_col: str = 'Close') -> pd.Series:
    """ìˆ˜ìµë¥  ê³„ì‚°"""
    return df[price_col].pct_change().fillna(0)

def calculate_volatility(returns: pd.Series, window: int = 20) -> pd.Series:
    """ë³€ë™ì„± ê³„ì‚°"""
    return returns.rolling(window=window).std()

def format_performance_metrics(metrics: dict) -> str:
    """ì„±ê³¼ ì§€í‘œ í¬ë§·íŒ…"""
    formatted = []
    for key, value in metrics.items():
        if isinstance(value, float):
            if 'ratio' in key.lower() or 'sharpe' in key.lower():
                formatted.append(f"**{key}**: {value:.4f}")
            elif 'return' in key.lower():
                formatted.append(f"**{key}**: {value:.2%}")
            else:
                formatted.append(f"**{key}**: {value:.4f}")
        else:
            formatted.append(f"**{key}**: {value}")
    return "\n".join(formatted)

def safe_division(numerator: Union[float, np.ndarray], denominator: Union[float, np.ndarray], default: float = 0.0) -> Union[float, np.ndarray]:
    """ì•ˆì „í•œ ë‚˜ëˆ—ì…ˆ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)"""
    if isinstance(denominator, (int, float)):
        return numerator / denominator if denominator != 0 else default
    else:
        return np.where(denominator != 0, numerator / denominator, default)

def create_date_range(start_date: str, end_date: str) -> Tuple[pd.Timestamp, pd.Timestamp]:
    """ë‚ ì§œ ë²”ìœ„ ìƒì„± ë° ê²€ì¦"""
    try:
        start = pd.to_datetime(start_date)
        end = pd.to_datetime(end_date)
        
        if start >= end:
            raise ValueError("ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")
        
        if end > pd.Timestamp.now():
            st.warning("ì¢…ë£Œì¼ì´ í˜„ì¬ ë‚ ì§œë³´ë‹¤ ë¯¸ë˜ì…ë‹ˆë‹¤. í˜„ì¬ ë‚ ì§œë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
            end = pd.Timestamp.now()
        
        return start, end
    except Exception as e:
        st.error(f"ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {e}")
        raise

def show_dataframe_info(df: pd.DataFrame, title: str = "ë°ì´í„° ì •ë³´"):
    """ë°ì´í„°í”„ë ˆì„ ì •ë³´ í‘œì‹œ"""
    with st.expander(f"ğŸ“Š {title}", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ë°ì´í„° ìˆ˜", len(df))
        
        with col2:
            st.metric("ì‹œì‘ì¼", df.index.min().strftime('%Y-%m-%d'))
        
        with col3:
            st.metric("ì¢…ë£Œì¼", df.index.max().strftime('%Y-%m-%d'))
        
        if isinstance(df, pd.DataFrame):
            for col in df.columns:
                if df[col].dtype == 'object':
                    df[col] = df[col].astype(str)
        st.dataframe(df.describe(), use_container_width=True)

def display_error_with_suggestions(error_msg: str, suggestions: List[str] = None):
    """ì—ëŸ¬ ë©”ì‹œì§€ì™€ í•´ê²° ë°©ì•ˆ í‘œì‹œ"""
    st.error(error_msg)
    
    if suggestions:
        with st.expander("ğŸ’¡ í•´ê²° ë°©ì•ˆ", expanded=True):
            for i, suggestion in enumerate(suggestions, 1):
                st.write(f"{i}. {suggestion}")

def show_success_with_details(success_msg: str, details: dict = None):
    """ì„±ê³µ ë©”ì‹œì§€ì™€ ìƒì„¸ ì •ë³´ í‘œì‹œ"""
    st.success(success_msg)
    
    if details:
        with st.expander("ğŸ“Š ìƒì„¸ ì •ë³´", expanded=False):
            for key, value in details.items():
                if isinstance(value, (int, float)):
                    if 'ratio' in key.lower() or 'sharpe' in key.lower():
                        st.write(f"**{key}**: {value:.4f}")
                    elif 'return' in key.lower():
                        st.write(f"**{key}**: {value:.2%}")
                    else:
                        st.write(f"**{key}**: {value:.4f}")
                else:
                    st.write(f"**{key}**: {value}")

def show_warning_with_help(warning_msg: str, help_text: str = None):
    """ê²½ê³  ë©”ì‹œì§€ì™€ ë„ì›€ë§ í‘œì‹œ"""
    st.warning(warning_msg)
    
    if help_text:
        with st.expander("â“ ë„ì›€ë§", expanded=False):
            st.info(help_text)

def validate_user_input(input_data: dict, required_fields: List[str]) -> Tuple[bool, List[str]]:
    """ì‚¬ìš©ì ì…ë ¥ ê²€ì¦"""
    errors = []
    
    for field in required_fields:
        if field not in input_data or input_data[field] is None:
            errors.append(f"'{field}' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        elif isinstance(input_data[field], str) and not input_data[field].strip():
            errors.append(f"'{field}' í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        elif isinstance(input_data[field], (list, tuple)) and len(input_data[field]) == 0:
            errors.append(f"'{field}' í•„ë“œì— ìµœì†Œ í•˜ë‚˜ì˜ í•­ëª©ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    return len(errors) == 0, errors

def show_progress_with_status(message: str, progress_bar=None):
    """ì§„í–‰ ìƒí™© í‘œì‹œ"""
    if progress_bar is None:
        progress_bar = st.progress(0)
    
    return progress_bar

def display_performance_summary(performance_metrics: dict, title: str = "ì„±ê³¼ ìš”ì•½"):
    """ì„±ê³¼ ì§€í‘œ ìš”ì•½ í‘œì‹œ"""
    st.subheader(title)
    
    # ì£¼ìš” ì§€í‘œë“¤ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
    return_metrics = {k: v for k, v in performance_metrics.items() if 'return' in k.lower()}
    risk_metrics = {k: v for k, v in performance_metrics.items() if any(x in k.lower() for x in ['volatility', 'drawdown', 'var'])}
    ratio_metrics = {k: v for k, v in performance_metrics.items() if any(x in k.lower() for x in ['sharpe', 'sortino', 'calmar'])}
    ic_metrics = {k: v for k, v in performance_metrics.items() if 'ic' in k.lower()}
    
    # ìˆ˜ìµë¥  ì§€í‘œ
    if return_metrics:
        st.markdown("**ğŸ“ˆ ìˆ˜ìµë¥  ì§€í‘œ**")
        col1, col2, col3 = st.columns(3)
        for i, (key, value) in enumerate(return_metrics.items()):
            with [col1, col2, col3][i % 3]:
                st.metric(key, f"{value:.2%}")
    
    # ë¦¬ìŠ¤í¬ ì§€í‘œ
    if risk_metrics:
        st.markdown("**âš ï¸ ë¦¬ìŠ¤í¬ ì§€í‘œ**")
        col1, col2, col3 = st.columns(3)
        for i, (key, value) in enumerate(risk_metrics.items()):
            with [col1, col2, col3][i % 3]:
                st.metric(key, f"{value:.4f}")
    
    # ë¹„ìœ¨ ì§€í‘œ
    if ratio_metrics:
        st.markdown("**ğŸ“Š ë¹„ìœ¨ ì§€í‘œ**")
        col1, col2, col3 = st.columns(3)
        for i, (key, value) in enumerate(ratio_metrics.items()):
            with [col1, col2, col3][i % 3]:
                st.metric(key, f"{value:.4f}")
    
    # IC ì§€í‘œ
    if ic_metrics:
        st.markdown("**ğŸ¯ IC ì§€í‘œ**")
        col1, col2, col3 = st.columns(3)
        for i, (key, value) in enumerate(ic_metrics.items()):
            with [col1, col2, col3][i % 3]:
                st.metric(key, f"{value:.4f}")

def create_download_link(data, filename: str, file_type: str = "csv"):
    """ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±"""
    import base64
    
    if file_type == "csv":
        csv = data.to_csv(index=True)
        b64 = base64.b64encode(csv.encode()).decode()
        href = f'<a href="data:file/csv;base64,{b64}" download="{filename}.csv">ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ</a>'
    elif file_type == "excel":
        import io
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            data.to_excel(writer, index=True)
        buffer.seek(0)
        b64 = base64.b64encode(buffer.getvalue()).decode()
        href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="{filename}.xlsx">ğŸ“¥ Excel ë‹¤ìš´ë¡œë“œ</a>'
    
    return href

def show_data_quality_report(df: pd.DataFrame, title: str = "ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸"):
    """ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ í‘œì‹œ"""
    st.subheader(title)
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ í–‰ ìˆ˜", len(df))
    
    with col2:
        st.metric("ì´ ì—´ ìˆ˜", len(df.columns))
    
    with col3:
        missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
        st.metric("ê²°ì¸¡ê°’ ë¹„ìœ¨", f"{missing_pct:.2f}%")
    
    with col4:
        duplicate_rows = df.duplicated().sum()
        st.metric("ì¤‘ë³µ í–‰", duplicate_rows)
    
    # ê²°ì¸¡ê°’ ìƒì„¸
    if df.isnull().any().any():
        st.markdown("**ğŸ” ê²°ì¸¡ê°’ ìƒì„¸**")
        missing_data = df.isnull().sum()
        missing_data = missing_data[missing_data > 0]
        st.bar_chart(missing_data)
    
    # ë°ì´í„° íƒ€ì… ì •ë³´
    st.markdown("**ğŸ“‹ ë°ì´í„° íƒ€ì… ì •ë³´**")
    dtype_info = pd.DataFrame({
        'ì»¬ëŸ¼ëª…': df.columns,
        'ë°ì´í„° íƒ€ì…': df.dtypes,
        'ê³ ìœ ê°’ ìˆ˜': df.nunique(),
        'ê²°ì¸¡ê°’ ìˆ˜': df.isnull().sum()
    })
    if isinstance(dtype_info, pd.DataFrame):
        for col in dtype_info.columns:
            if dtype_info[col].dtype == 'object':
                dtype_info[col] = dtype_info[col].astype(str)
    st.dataframe(dtype_info, use_container_width=True)

def cache_key_generator(*args) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    return "_".join(str(arg) for arg in args)

@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ
def cached_calculation(func, *args, **kwargs):
    """ê³„ì‚° ê²°ê³¼ ìºì‹±"""
    return func(*args, **kwargs)

def save_factor_to_zoo(factor_name: str, factor_data: dict, zoo_dir: str = 'factor_zoo'):
    """
    íŒ©í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ factor_zoo/ì— pickleë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    - factor_name: ì €ì¥í•  íŒŒì¼ëª…(í™•ì¥ì ì œì™¸)
    - factor_data: {'meta': ë©”íƒ€ë°ì´í„° dict, 'factor': íŒ©í„° DataFrame/Series ë“±}
    - zoo_dir: ì €ì¥ í´ë”
    """
    if not os.path.exists(zoo_dir):
        os.makedirs(zoo_dir)
    file_path = os.path.join(zoo_dir, f"{factor_name}.pkl")
    with open(file_path, 'wb') as f:
        pickle.dump(factor_data, f)


def load_factors_from_zoo(zoo_dir: str = 'factor_zoo') -> dict:
    """
    factor_zoo/ ë‚´ ëª¨ë“  íŒ©í„° ëª©ë¡ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë°˜í™˜: {factor_name: {'meta': ..., 'factor': ...}}
    """
    factors = {}
    if not os.path.exists(zoo_dir):
        return factors
    for fname in os.listdir(zoo_dir):
        if fname.endswith('.pkl'):
            fpath = os.path.join(zoo_dir, fname)
            try:
                with open(fpath, 'rb') as f:
                    data = pickle.load(f)
                    factors[fname[:-4]] = data
            except Exception as e:
                logger.warning(f"íŒ©í„° Zoo ë¡œë“œ ì‹¤íŒ¨: {fname} - {e}")
    return factors


def delete_factor_from_zoo(factor_name: str, zoo_dir: str = 'factor_zoo'):
    """
    factor_zoo/ ë‚´ íŠ¹ì • íŒ©í„° íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    - factor_name: ì‚­ì œí•  íŒŒì¼ëª…(í™•ì¥ì ì œì™¸)
    """
    file_path = os.path.join(zoo_dir, f"{factor_name}.pkl")
    if os.path.exists(file_path):
        os.remove(file_path)

load_dotenv()
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

def summarize_with_llm(text: str, prompt: str = "", api_key: str = None) -> str:
    """
    OpenAI gpt-4.1-mini-2025-04-14 ê¸°ë°˜ ìì—°ì–´ í•´ì„ í•¨ìˆ˜. .envì—ì„œ í‚¤ë¥¼ ì½ì–´ ë³´ì•ˆ ìœ ì§€.
    - text: í•´ì„í•  ë°ì´í„°/ì§€í‘œ ìš”ì•½ í…ìŠ¤íŠ¸
    - prompt: ì¶”ê°€ í”„ë¡¬í”„íŠ¸(ì˜µì…˜)
    - api_key: LLM API í‚¤(ì˜µì…˜, ì—†ìœ¼ë©´ .env ì‚¬ìš©)
    """
    key = api_key or OPENAI_API_KEY
    if not key:
        return None
    try:
        openai.api_key = key
        system_prompt = prompt or "ì•„ë˜ ë°ì´í„°ë¥¼ íˆ¬ì ì „ë¬¸ê°€ ê´€ì ì—ì„œ í•´ì„/ì œì•ˆí•´ì¤˜."
        user_content = f"ë¶„ì„ ë°ì´í„°:\n{text}"
        response = openai.chat.completions.create(
            model="gpt-4o-mini",  # ìœ íš¨í•œ GPT-4 mini ëª¨ë¸
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            max_tokens=400,
            temperature=0.5
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[LLM í•´ì„ ì˜¤ë¥˜] {e}"

def analyze_factor_performance_text(perf: dict, llm_api_key: str = None) -> str:
    """
    íŒ©í„° ì„±ëŠ¥(performance dict)ì„ í•´ì„í•˜ê³  íˆ¬ìì  ì‹œì‚¬ì /ì œì•ˆì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    LLM API í‚¤ê°€ ìˆìœ¼ë©´ LLM í•´ì„ì„ ìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    if not perf:
        return "íŒ©í„° ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    # LLM í•´ì„ ìš°ì„ 
    if llm_api_key:
        llm_result = summarize_with_llm(str(perf), prompt="íŒ©í„° ì„±ëŠ¥ì„ íˆ¬ì ê´€ì ì—ì„œ í•´ì„/ì œì•ˆí•´ì¤˜.", api_key=llm_api_key)
        if llm_result:
            return llm_result
    # ì´í•˜ rule-based í•´ì„
    mean_ic = perf.get('mean_ic', 0)
    icir = perf.get('icir', 0)
    spread = perf.get('factor_spread', 0)
    turnover = perf.get('factor_turnover', 0)
    msg = []
    # IC í•´ì„
    if mean_ic > 0.05:
        msg.append(f"âœ… í‰ê·  IC({mean_ic:.4f})ê°€ 0.05 ì´ìƒìœ¼ë¡œ ì˜ˆì¸¡ë ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤.")
    elif mean_ic > 0.02:
        msg.append(f"â„¹ï¸ í‰ê·  IC({mean_ic:.4f})ê°€ 0.02~0.05ë¡œ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤. íŒ©í„° ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
    else:
        msg.append(f"âš ï¸ í‰ê·  IC({mean_ic:.4f})ê°€ ë‚®ì•„ ì˜ˆì¸¡ë ¥ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. íŒ©í„° êµ¬ì¡°/íŒŒë¼ë¯¸í„°ë¥¼ ì¬ì ê²€í•˜ì„¸ìš”.")
    # ICIR í•´ì„
    if icir > 1:
        msg.append(f"âœ… ICIR({icir:.2f})ê°€ 1 ì´ìƒìœ¼ë¡œ ì˜ˆì¸¡ë ¥ì˜ ì¼ê´€ì„±/ì•ˆì •ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.")
    elif icir > 0.5:
        msg.append(f"â„¹ï¸ ICIR({icir:.2f})ê°€ 0.5~1ë¡œ ì–‘í˜¸í•©ë‹ˆë‹¤.")
    else:
        msg.append(f"âš ï¸ ICIR({icir:.2f})ê°€ ë‚®ì•„ íŒ©í„°ì˜ ì¼ê´€ì„±ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    # ë¶„ì‚°/í„´ì˜¤ë²„ í•´ì„
    if spread < 0.05:
        msg.append(f"âš ï¸ íŒ©í„° ë¶„ì‚°({spread:.4f})ì´ ë‚®ì•„ ì¢…ëª© ê°„ ì°¨ë³„í™”ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    if turnover > 0.5:
        msg.append(f"âš ï¸ íŒ©í„° í„´ì˜¤ë²„({turnover:.2f})ê°€ ë†’ì•„ ê±°ë˜ë¹„ìš© ì¦ê°€ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.")
    # ì¢…í•© ì œì•ˆ
    if mean_ic > 0.05 and icir > 1:
        msg.append("ğŸ¯ ì´ íŒ©í„°ëŠ” ì‹¤ì œ íˆ¬ì ì „ëµì— ë°”ë¡œ ì ìš©í•´ë³¼ ë§Œí•©ë‹ˆë‹¤.")
    elif mean_ic > 0.02:
        msg.append("ğŸ” íŒ©í„° íŒŒë¼ë¯¸í„° íŠœë‹, ê²°í•©, ë”¥ëŸ¬ë‹ íŒ©í„°ì™€ì˜ ì¡°í•©ì„ ì¶”ê°€ ì‹¤í—˜í•´ë³´ì„¸ìš”.")
    else:
        msg.append("ğŸ› ï¸ íŒ©í„° êµ¬ì¡°/ë°ì´í„°/íŒŒë¼ë¯¸í„°ë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ ì¬ì„¤ê³„í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.")
    return '\n'.join(msg)

def analyze_backtest_performance_text(report: dict, llm_api_key: str = None) -> str:
    """
    ë°±í…ŒìŠ¤íŠ¸ ì„±ê³¼ ë¦¬í¬íŠ¸(ì„±ê³¼ì§€í‘œ dict)ë¥¼ í•´ì„í•˜ê³  íˆ¬ìì  ì‹œì‚¬ì /ì œì•ˆì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    LLM API í‚¤ê°€ ìˆìœ¼ë©´ LLM í•´ì„ì„ ìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    if not report:
        return "ë°±í…ŒìŠ¤íŠ¸ ì„±ê³¼ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    if llm_api_key:
        llm_result = summarize_with_llm(str(report), prompt="ë°±í…ŒìŠ¤íŠ¸ ì„±ê³¼ë¥¼ íˆ¬ì ê´€ì ì—ì„œ í•´ì„/ì œì•ˆí•´ì¤˜.", api_key=llm_api_key)
        if llm_result:
            return llm_result
    sharpe = report.get('Sharpe', 0)
    ret = report.get('Return', 0)
    maxdd = report.get('MaxDrawdown', 0)
    msg = []
    # Sharpe í•´ì„
    if sharpe > 1.5:
        msg.append(f"âœ… ìƒ¤í”„ ë¹„ìœ¨({sharpe:.2f})ì´ 1.5 ì´ìƒìœ¼ë¡œ ìœ„í—˜ ëŒ€ë¹„ ìˆ˜ìµì´ ë§¤ìš° ìš°ìˆ˜í•©ë‹ˆë‹¤.")
    elif sharpe > 1.0:
        msg.append(f"â„¹ï¸ ìƒ¤í”„ ë¹„ìœ¨({sharpe:.2f})ì´ 1.0~1.5ë¡œ ì–‘í˜¸í•©ë‹ˆë‹¤.")
    else:
        msg.append(f"âš ï¸ ìƒ¤í”„ ë¹„ìœ¨({sharpe:.2f})ì´ ë‚®ì•„ ì „ëµì˜ ìœ„í—˜ ëŒ€ë¹„ ìˆ˜ìµì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    # ìˆ˜ìµë¥  í•´ì„
    if ret > 0.2:
        msg.append(f"âœ… ì—°í™˜ì‚° ìˆ˜ìµë¥ ({ret:.2%})ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.")
    elif ret > 0.05:
        msg.append(f"â„¹ï¸ ì—°í™˜ì‚° ìˆ˜ìµë¥ ({ret:.2%})ì´ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
    else:
        msg.append(f"âš ï¸ ì—°í™˜ì‚° ìˆ˜ìµë¥ ({ret:.2%})ì´ ë‚®ì•„ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    # MDD í•´ì„
    if maxdd > 0.3:
        msg.append(f"âš ï¸ ìµœëŒ€ ë‚™í­({maxdd:.2%})ì´ ì»¤ì„œ ë¦¬ìŠ¤í¬ ê´€ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    elif maxdd > 0.15:
        msg.append(f"â„¹ï¸ ìµœëŒ€ ë‚™í­({maxdd:.2%})ì´ ë‹¤ì†Œ í½ë‹ˆë‹¤. ë°©ì–´ ì „ëµì„ ê³ ë ¤í•˜ì„¸ìš”.")
    else:
        msg.append(f"âœ… ìµœëŒ€ ë‚™í­({maxdd:.2%})ì´ ìš°ìˆ˜í•˜ê²Œ ê´€ë¦¬ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
    # ì¢…í•© ì œì•ˆ
    if sharpe > 1.5 and ret > 0.1 and maxdd < 0.15:
        msg.append("ğŸ¯ ì´ ì „ëµì€ ì‹¤ì œ íˆ¬ìì— ë§¤ìš° ì í•©í•©ë‹ˆë‹¤.")
    elif sharpe > 1.0:
        msg.append("ğŸ” ë¦¬ë°¸ëŸ°ì‹± ì£¼ê¸°, ê±°ë˜ë¹„ìš©, íŒ©í„° ì¡°í•© ë“± ì¶”ê°€ ì‹¤í—˜ì„ ì¶”ì²œí•©ë‹ˆë‹¤.")
    else:
        msg.append("ğŸ› ï¸ íŒ©í„°/ì „ëµ êµ¬ì¡°, ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë°©ì•ˆì„ ì¬ì„¤ê³„í•´ë³´ì„¸ìš”.")
    return '\n'.join(msg)
